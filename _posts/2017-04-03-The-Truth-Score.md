---
title: The Truth Score
layout: post
comments: true
date: 2017-04-03 10:00:00 +GMT
author: Sudeep Agarwal
---
In *The Power of News*, author Michael Schudson writes that news is 'a form of culture' and argues that it 'changes the character of knowledge', allowing us to act on that knowledge in new and significant ways. But what happens then, if the news that we read is interspersed with lies and factual inaccuracies? Are we acting behind a facade of misinformation, changing our values and beliefs, and making decisions based on information that is meant to deceive? This might not make a huge difference on our daily actions but when it comes to events such as the recent US elections, every piece of information is critical.

We often have a tendency to believe what we read. This becomes a problem especially on the Internet where one has the power and ability to write and publish anything without verification or proof. What exacerbates this issue, however, is not the quantity of misinformation that exists, but rather the virality of the misinformation. Mike Caulfield, the director of networked learning at Washington State University, published some findings that suggest that fake news could in fact be more viral than real news. This is not surprising - fake news is often published with an agenda and could incite a more emotional response, causing people to share it with others. In my response to this question, I will suggest a strategy that could help curb the spread of misinformation.

### Measuring Accuracy
According to a study conducted by Pew Research Center, nearly 62% of U.S. adults get their news from social media, and 66% of Facebook users get news from the site. With almost 1.18 billion daily active users, a significant proportion of the world depends on Facebook for their news. With great power comes great responsibility, and I believe that Facebook should ensure that the knowledge that users are obtaining through the platform should be as accurate as possible. While it may be impossible to completely curb all inaccuracies, one possible approach could be to design a *truth score* that predicts how factual a news article is. This score could be used to warn the user that the article they're about to read or the link they're to click on may contain inaccurate content. This would allow the user to make a more informed decision on whether to believe what they are about to read. Designing such a score would have to take into account multiple factors. I'd like to suggest some which could contribute to the score.

**Machine learning techniques** - We can use methods like deep learning and random forests to understand how to categorize existing fake news so that we can predict the accuracy of any new content. Given the exceptional amount of data that Facebook has access to, creating a prediction model with substantial accuracy would not be impossible.

**Computational fact checking** - Using natural language processing techniques such as keyword extraction, relationship extraction and sentiment analysis, an algorithm can form an understanding of the article's propositions. In the research article *Computational Fact Checking from Knowledge Networks*, the authors propose that we can use the shortest distance between concept nodes on knowledge graphs to approximate the accuracy of articles.

**User reporting** - While this is something that Facebook already allows on its platform, I would hypothesize that a majority of users are likely to be unaware that the article is inaccurate till it has become viral, at which point there has already been significant damage done. If the previous components are implemented well, this could potentially just become a fallback for content verification.

### Risks and Analyzing Success
One approach that could be used to test the success of the truth score is to track the virality of fake content. If the truth score is in fact informing users of the inaccuracies in the content they are viewing, there is likely to be a downward trend in the virality of such content as users will be less likely to like or share it. The results of the truth score could also be compared by the verification of third party fact checking organizations. The corroboration of these results with the predictions derived using the score could add to its validity.

It is important, I think, as Mark Zuckerberg posted regarding the issue of misinformation, for Facebook not to be `arbiters of truth' themselves. Facebook has always been an open online community where anyone can express their opinions. Restricting content might bring up issues of censorship and divert from Facebook's mission. Therefore, I think that the focus of the truth score should not be on removing inaccurate content, but on reminding users the importance of understanding the accuracy of knowledge they are gaining online. If we are aware that the content we consume may perhaps be false, we can do the due diligence by gaining more information rather than taking everything as is.